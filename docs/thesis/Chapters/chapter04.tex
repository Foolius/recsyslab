
\chapter{Recommendation Algorithms}
\label{recommendationalgorithms}
In this chapter we will provide an overview on how the algorithms we have implemented
work. And show the core part of their source code in recsyslab to illustrate the 
simplicity of the code. For further explanations please refer to the cited papers.


\section{Non-Personalized Algorithms}

In this chapter we will describe two very simple and basic recommendation
algorithms we implemented for comparison with the more sophisticated
algorithms.


\subsection{Constant}

The constant recommender algorithm counts the number of interactions
for each item and sorts this in decreasing order of interactions.
Then it recommends the top items of this list. So it recommends the
items which are the most popular over all users and does not do any
personalizations. The intuition here is that the most popular items
will be interesting for everyone. However the results show that
algorithms which personalize the recommendation based on the 
interaction history of the users perform much better.
In pseudocode this algorithm will look like this:
\begin{lstlisting}
countDict = dict initialized with all ItemIDs as keys and 0 as values

for interaction in dataset:
    countDict(ItemID(interaction))++

ranking = list(ItemIDs in decreasing order of their values)

return first N items of ranking
\end{lstlisting}
And the implementation:
\begin{lstlisting}
def __init__(self, dbdict):
    self.dictionary = {}
    self.sortedList = []
    for data in dbdict.iteritems():
        for item, rating in iter(data[1]):
            if item in self.dictionary:
                self.dictionary[item] += rating
            else:
                self.dictionary[item] = rating

    self.sortedList = helper.sortList(self.dictionary.iteritems())

def getRec(self, user, n):
    return self.sortedList[:n]
\end{lstlisting}



\subsection{Random}

The random recommender algorithm chooses items to recommend randomly.
Even though this algorithm will recommend different items for different
users it is not personalized because it the recommendations are
independent of the previous interactions of the user.
In pseudocode this will be:
\begin{lstlisting}
return N randomly chosen items
\end{lstlisting}
And in Python:
\begin{lstlisting}
def __init__(self, dbdict, seed):
    self.maxIid = 0
    self.seed = seed
    for data in dbdict.iteritems():
        for itemRating in iter(data[1]):
            item = itemRating[0]
            if item > self.maxIid:
                self.maxIid = item
    self.maxIid += 1

def getRec(self, user, n):
    random.seed(self.seed)
    if self.maxIid < n or n == -1:
        l = range(self.maxIid)
        random.shuffle(l)
        return l
    return list(random.sample(range(self.maxIid), n))
\end{lstlisting}


\section{k-Nearest-Neighbor}

This class of recommendation algorithms works by searching neighbors
of either items or users based on a similarity function which is the
cosine in this library. The similarity function is interchangeable 
for example in
\cite{Karypis:2001:EIT:502585.502627} two similarity functions are
compared. The cosine similarity performs best so in recsyslab only the cosine similarity
is implemented. For two vectors \(\overrightarrow{v},\overrightarrow{u}\)
The cosine similarity is defined as follows:

\begin{equation}
\cos(\overrightarrow{v}, \overrightarrow{u})=\frac{\overrightarrow{v} \cdot \overrightarrow{u}}{||\overrightarrow{v}||_{2} ||\overrightarrow{u}||_{2}}
\end{equation}



\subsection{Item Based}
\label{itembasedknn}

For this algorithm the database has to be represented as a matrix
where the rows correspond to the users and the columns to the items.
Then the entry (i,j) represents the number of transactions which happened
between the ith user and the jth item. 

The algorithm interprets the columns of the matrix i.e. the items
as vectors and computes there similarities by computing their cosine.
To build the model the algorithm computes the n most similar items
of each item. In pseudocode:
\begin{lstlisting}
for every item i
    for every item j
        sim[i,j] = similarity between i and j

for every item i
    for every item j
        if sim[i,j] not one of the n largest in sim[i]
            sim[i,j] = 0
\end{lstlisting}
In Python it looks like this:
\begin{lstlisting}
def __init__(self, userItemMatrix, n):
    self.itemUserMatrix = userItemMatrix.transpose()
    self.sim = computeCosSim(self.sim, self.itemUserMatrix)
    self.userItemMatrix = userItemMatrix

    order = self.sim.argsort(1)

    # for each row in sim:
    # Set all entries to 0 except the n highest
    for j in xrange(0, self.sim.shape[1]):
        for i in xrange(0, self.sim.shape[1] - n):
            self.sim[j, order[j, i]] = 0
\end{lstlisting}

To compute recommendations for user U the algorithm computes
the union of the n most similar items of each item U interacted with.
From this set the items U already interacted with are removed. For
each item remaining in this set we compute the sum of its similarities
to the items U interacted with. Finally these items are sorted in
decreasing order of this sum of similarities and the first n items
will be recommended~\cite{Karypis:2001:EIT:502585.502627}.
The pseudocode is
\begin{lstlisting}
for every item i user u bought
    itemSimVector = itemSimVector + vector of i

for every item i user u bought
    itemSimVector[i] = 0

return the N items with the highest value in itemSimVector
\end{lstlisting



\subsection{User Based}

The user based k-Nearest-Neighbor~\cite{userbasedknn} is very similar to the item based.
But instead of interpreting the columns as vectors we interpret the
lines or users of the matrix as vectors and compute their similarities
to other users.

Then for each item i we sum up the similarities between U and the
users who interacted with i. Again we remove all items U already interacted
with, sort in decreasing order fo the sum and recommend the first
n items.


\section{Matrix Factorization}

All matrix factorization techniques build two matrices in the model
building phase. These matrices are supposed to represent abstract
features of each item and user. For recommendation the dot product
of the feature vector of an user and an item gives a score whith which
we can sort the items and recommend the best suitable ones. The process
of presenting a large matrix \(M\) as two smaller matrices \(W\) and \(H\) so
that \(M = W \times H\) is also called singular value decomposition.

Each of the implemented algorithms train the model with stochastic
gradient descent. In each iteration the model is trained with a randomly
chosen user, a randomly chosen item the user interacted with, called
the positive item and a randomly chosen item the user did not interacted
with yet, called the negative item. The features of the user and the
negative and the positive item are then trained according to the derivative
of a loss function.


\subsection{BPRMF}

BPMRF uses the logloss to train the model. The logloss is defined
as
\[
\textrm{logLoss}(a,y)=\log(1+\exp(-ay))
\]


And the derivative of the log loss is
\[
\frac{\partial}{\partial y}(\log(1+\exp(-ay)))=-\frac{a}{\exp(ay)+1}
\]


For further informations please refer to~\cite{Rendle:2009:BBP:1795114.1795167}


\subsection{RankMFX}

RankMFX uses the hingeLoss. It is defined as

\[
\mathrm{\textrm{hingeLoss}(a,y)=\max(0,1-ay)}
\]


And its derivative

\[
\frac{\partial}{\partial y}(\max(0,1-ay))=\begin{cases}
-y & ay<1\\
0 & \textrm{otherwise}
\end{cases}
\]


See also~\cite{diaz2012happening}.


\subsection{Ranking SVD (Sparse SVD)}

Ranking SVD uses the quadratic loss and the difference between the
predicted score of the positive item and the negative minus the actual
score of the positive item~\cite{jahrer2011collaborative}.

\subsection{Slope One}

The Slope One recommendation algorithm computes the differences of
interaction intensity between items and uses these differences to 
predict indirectly the interaction intensity between users and items
without any interaction yet~\cite{DBLP:journals/corr/abs-cs-0702144}.
