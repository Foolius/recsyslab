
\chapter{Recommendation Algorithms}
\label{recommendationalgorithms}
In this chapter I will roughly explain how the algorithms I've implemented
work. For further explanations please refer to the cited papers.


\section{Non-Personalized Algorithms}

In this chapter I will describe two very simple and basic recommendation
algorithms I implemented for comparison with the more sophisticated
algorithms.


\subsection{Constant}

The constant recommender algorithm counts the number of interactions
for each item and sorts this in decreasing order of interactions.
Then it recommends the top items of this list. So it recommends the
items which are the most popular over all users and doesn't do any
personalizations.


\subsection{Random}

The random recommender algorithm chooses items to recommend randomly.
Even though this algorithm will recommend different items for different
users it is not personalized because it the recommendations are
independent of the previous interactions of the user.


\section{k-Nearest-Neighbor}

This class of recommendation algorithms works by searching neighbors
of either items or users based on a similarity function which is the
cosine in this library.


\subsection{Item Based}
\label{itembasedknn}

For this algorithm the database has to be represented as a matrix
where the rows correspond to the users and the columns to the items.
Then the entry (i,j) represents the number of transactions which happened
between the ith user and the jth item. 

The algorithm interprets the columns of the matrix i.e. the items
as vectors and computes there similarities by computing their cosine.
To build the model the algorithm computes the n most similar items
of each item. 

To compute recommendations for user U the algorithm then computes
the union of the n most similar items of each item U interacted with.
From this set the items U already interacted with are removed. For
each item remaining in this set we compute the sum of its similarities
to the items U interacted with. Finally these items are sorted in
decreasing order of this sum of similarities and the first n items
will be recommended~\cite{Karypis:2001:EIT:502585.502627}.


\subsection{User Based}

The user based k-Nearest-Neighbor is very similar to the item based.
But instead of interpreting the columns as vectors we interpret the
lines or users of the matrix as vectors and compute their similarities
to other users.

Then for each item i we sum up the similarities between U and the
users who interacted with i. Again we remove all items U already interacted
with, sort in decreasing order fo the sum and recommend the first
n items.


\section{Matrix Factorization}

All matrix factorization techniques build two matrices in the model
building phase. These matrices are supposed to represent abstract
features of each item and user. For recommendation the dot product
of the feature vector of an user and an item gives a score whith which
we can sort the items and recommend the best suitable ones. The process
of presenting a large matrix M as two smaller matrices W and H so
that M = W{*}H is also called singular value decomposition.

Each of the implemented algorithms train the model with stochastic
gradient descent. In each iteration the model is trained with a randomly
chosen user, a randomly chosen item the user interacted with, called
the positive item and a randomly chosen item the user didn't interacted
with yet, called the negative item. The features of the user and the
negative and the positive item are then trained according to the derivative
of a loss function.


\subsection{BPRMF}

BPMRF uses the logloss to train the model. The logloss is defined
as
\[
\textrm{logLoss}(a,y)=\log(1+\exp(-ay))
\]


And the derivative of the log loss is
\[
\frac{\partial}{\partial y}(\log(1+\exp(-ay)))=-\frac{a}{\exp(ay)+1}
\]


For further informations please refer to~\cite{Rendle:2009:BBP:1795114.1795167}


\subsection{RankMFX}

RankMFX uses the hingeLoss. It is defined as

\[
\mathrm{\textrm{hingeLoss}(a,y)=\max(0,1-ay)}
\]


And its derivative

\[
\frac{\partial}{\partial y}(\max(0,1-ay))=\begin{cases}
-y & ay<1\\
0 & \textrm{otherwise}
\end{cases}
\]


See also {[}Paper for citation?{]}


\subsection{Ranking SVD (Sparse SVD)}

Ranking SVD uses the quadratic loss and the difference between the
predicted score of the positive item and the negative minus the actual
score of the positive item~\cite{jahrer2011collaborative}.

\subsection{Slope One}

The Slope One recommendation algorithm computes the differences of
interaction intensity between items and uses these differences to 
predict indirectly the interaction intensity between users and items
without any interaction yet~\cite{DBLP:journals/corr/abs-cs-0702144}.
