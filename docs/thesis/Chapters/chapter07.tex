\chapter{User Manual}
\label{usermanual}
In this chapter we provide a step by step user manual for recsyslab.
You can get recsyslab at \url{https://github.com/Foolius/recsyslab/archive/master.zip}.
Simply download the file and unpack it.
To run recsyslab you need to install Python~\cite{python} and NumPy~\cite{numpy}.
Please refer to their websites for information on how to install them.

First, we will explain how to load a dataset and where to get one. Second, we
will explain how to use the different recommendation algorithms and
how to test them with the provided evaluation metrics. 
For information about the recommendation algorithms, please refer to Chapter~\ref{recommendationalgorithms}.
For information about the test metrics refer to Chapter~\ref{background}.

\section{Datasets for testing}
\label{movielens}

MovieLens~\cite{movielensdatasets} is a database provided online by GroupLens, a research
lab at the University of Minnesota. One of their research areas is
recommender systems and they have built an application where users rate
movies and then get recommendations for movies they could potentially like. The
MovieLens dataset is a collection of the ratings gathered by this application. For
this work, we will interpret the rating as intensity of interaction
between users and items. For example, the number of times the user watched a movie.

The dataset is available in three different sizes:
\begin{itemize}
\item 100,000 interactions
\item 1 million interactions
\item 10 million interactions
\end{itemize}
For the experiments, the smallest dataset is totally sufficient, with
the larger datasets the computation time increases largely.

There are also other datasets available, for example the 
million song dataset~\cite{Bertin-Mahieux2011}. But in the examples below, we use the 
MovieLens dataset.

\section{Load the Dataset}
The following Python code is also in the file \lstinline!manual.py!, in the \lstinline!bin! directory of the library,
so you do not have to copy the code out of this PDF document. But if you are working through these examples,
please note that you might miss the import of a module when you skip an example, because
we do not specify every import needed in each new example.
For help, you can also use the inline documentation, which is available as docstrings.
You can display them with the command line utility\footnote{We will use \lstinline!$! to indicate a bash prompt.} pydoc.
When you are in the bin directory of the library you might call

\begin{lstlisting}
$ pydoc __init__
$ pydoc util 
$ pydoc recommender.BPRMF
\end{lstlisting}
Each of these commands will show the documentation of the specified module.
The docstrings are also available in the appendix of this thesis.


In Chapter~\ref{design}, we explained how the dataset has to look like.
For trying things out the MovieLens dataset~\cite{movielensdatasets} would work just fine.
When you have a dataset, place it into the bin directory of recsyslab and start the
python interpreter from the command line with
\begin{lstlisting}
$ python
\end{lstlisting}
Now import the \lstinline!util.reader! module and initialize a new reader object with
\footnote{We will use \lstinline!>>>! to indicate the python prompt.}
\begin{lstlisting}[style=python]
>>> import util.reader
>>> r=util.reader.stringSepReader("u.data","\t")
Start reading the database.
10000 Lines read.
20000 Lines read.
30000 Lines read.
40000 Lines read.
50000 Lines read.
60000 Lines read.
70000 Lines read.
80000 Lines read.
90000 Lines read.
100000 Lines read.
\end{lstlisting}
Note that it outputs the progress it has already made.
The first argument of the constructor is the name of the file containing the dataset,
the second the string which is separating the values, here it is a tab. 
When you are using another dataset, you probably have to change the filename and perhaps
also the separating string.

\subsubsection*{Mapping}
The constructor creates a mapping from the original IDs to internal IDs both for the users
and the items to make sure that the IDs are consecutive. So to get the items a user interacted
with, we have to find out the internal UserID first. 
\begin{lstlisting}[style=python]
>>> internalID=r.getInternalUid("196")
>>> r.getR()[internalID]
set([(521, 1), (377, 4), (365, 3), (438, 5), (86, 4), (649, 4), (0, 3), (522, 3), (423, 3), (389, 5), (751, 3), (656, 4), (947, 4), (432, 2), (632, 2), (431, 5), (221, 5), (92, 4), (291, 3), (528, 4), (83, 4), (363, 3), (466, 4), (289, 5), (512, 5), (179, 3), (329, 4), (672, 4), (834, 5), (665, 3), (321, 2), (487, 3), (380, 4), (1006, 4), (1045, 3), (491, 3), (302, 4), (550, 5), (10, 2)])
\end{lstlisting}
\lstinline!r.getR()! returns a dict with internal UserIDs as keys and sets of (ItemID, NumberOfInteractions) tuples.
Please note that the original ID is a string.

To evaluate the algorithms, you have to split the datasets like described at \ref{leaveoneout}.
You do this by calling
\begin{lstlisting}[style=python]
>>> import util.split
>>> trainingDict, evaluationDict = util.split(r.getR(), 1234567890)
0 Users split.
100 Users split.
200 Users split.
300 Users split.
400 Users split.
500 Users split.
600 Users split.
700 Users split.
800 Users split.
900 Users split.
\end{lstlisting}
This will split a dict like \lstinline!r.getR()! returns into a \lstinline!trainingDict! where one transaction
per user is missing and an \lstinline!evaluationDict! with these missing transactions.


\section{Non-Personalized Algorithms}
To make our first simple recommendations with the constant recommender,
we need to initialize an object of the constant class.
As an argument the constructor needs a dict for training:
\begin{lstlisting}[style=python]
>>> import recommender.nonpersonalized
>>> constant = recommender.nonpersonalized.constant(trainingDict)
>>> constant.getRec(0, 10)
[357, 49, 52, 157, 101, 24, 289, 189, 31, 60]
\end{lstlisting}
Every recommender has a \lstinline!getRec! function with this signature. The first argument is the internal
UserID, the second is the number of items to be recommended. If n = -1, all items get recommendend.
The IDs of the recommended items are internal ones, too.
If you want to pass external UserIDs and get external ItemIDs back, you do not have
to map them all by yourself. Instead, you can use a helper function called
\lstinline!getExternalRec!.
\begin{lstlisting}[style=python]
>>> import util.helper
>>> externalConstantgetRec = 
    util.helper.getExternalRec(constant.getRec, r)
>>> externalConstantgetRec("196", 10)
['50', '100', '181', '258', '174', '1', '286', '127', '98', '288']
\end{lstlisting}
You pass the \lstinline!getRec! function and the reader object and you get a new function
which takes and returns only the original IDs.

Now, we want to find out how well this algorithm is performing.
Except AUC, the functions for computing the metrics work by letting the recommender recommend
for every user as much items as specified in the third argument.
With these recommendations and the hidden items, the methods calculate the
performance, according to the respective metric.
The other arguments are a dict with the hidden items and the \lstinline!getRec! function of the 
respective recommender algorithm.\\
First we will compute the hitrate:
\begin{lstlisting}[style=python]
>>> import util.test
>>> util.test.hitrate(evaluationDict, constant.getRec, 10)
0 users tested
Hits so far: 0.0
100 users tested
Hits so far: 4.0
200 users tested
Hits so far: 14.0
300 users tested
Hits so far: 26.0
400 users tested
Hits so far: 31.0
500 users tested
Hits so far: 40.0
600 users tested
Hits so far: 49.0
700 users tested
Hits so far: 52.0
800 users tested
Hits so far: 61.0
900 users tested
Hits so far: 67.0
Number of hits: 69.0
Number of possible hits: 943.0
Hitrate: 0.07317073170731707
\end{lstlisting}
After some progress output it prints the number of hits and the number of 
possible hits which is in our case also the number of users. The hitrate
is then the number of hits over the number of users.
Next, we will calculate the precision:
\begin{lstlisting}[style=python]
>>> util.test.precision(evaluationDict, constant.getRec, 10)
0 users tested
Hits so far: 0.0
100 users tested
Hits so far: 4.0
200 users tested
Hits so far: 14.0
300 users tested
Hits so far: 26.0
400 users tested
Hits so far: 31.0
500 users tested
Hits so far: 40.0
600 users tested
Hits so far: 49.0
700 users tested
Hits so far: 52.0
800 users tested
Hits so far: 61.0
900 users tested
Hits so far: 67.0
Number of hits: 69.0
Number of possible hits: 943.0
Precision: 6.9
\end{lstlisting}
The output looks similar to the one of hitrate and the meaning is analogous.\\
Very similarly we can calculate the F1 metric:
\begin{lstlisting}[style=python]
>>> util.test.f1(evaluationDict, constant.getRec, 10)
0 users tested
Hits so far: 0.0
100 users tested
Hits so far: 4.0
200 users tested
Hits so far: 14.0
300 users tested
Hits so far: 26.0
400 users tested
Hits so far: 31.0
500 users tested
Hits so far: 40.0
600 users tested
Hits so far: 49.0
700 users tested
Hits so far: 52.0
800 users tested
Hits so far: 61.0
900 users tested
Hits so far: 67.0
Number of hits: 69.0
Number of possible hits: 943.0
F1: 0.14480587618048268
\end{lstlisting}
Also the mean reciprocal hitrate:
\begin{lstlisting}[style=python]
>>> util.test.mrhr(evaluationDict, constant.getRec, 10)
0 users tested
Score so far: 0.0
100 users tested
Score so far: 0.675
200 users tested
Score so far: 2.9972222222222227
300 users tested
Score so far: 7.290079365079365
400 users tested
Score so far: 8.34404761904762
500 users tested
Score so far: 10.305158730158729
600 users tested
Score so far: 12.464682539682537
700 users tested
Score so far: 12.982539682539679
800 users tested
Score so far: 18.332539682539675
900 users tested
Score so far: 19.56825396825396
Score: 19.79325396825396
Number of possible hits: 943.0
Mean Reciprocal Hitrate: 0.02098966486559275
\end{lstlisting}
To compute the AUC, the third argument has to be our \lstinline!reader! object because
the function needs additional information to compute its metric.
We do not need to specify the number of items to recommend, because
the AUC works by comparing all items. Thus, it needs the recommender
to recommend all available items.
The first two arguments do not change.
\begin{lstlisting}[style=python]
>>> util.test.auc(evaluationDict, constant.getRec, r)
0 users tested
Score so far: 0
100 users tested
Score so far: 77.97155778327935
200 users tested
Score so far: 163.24761858534592
300 users tested
Score so far: 247.0700325949549
400 users tested
Score so far: 329.3602045603418
500 users tested
Score so far: 413.3631291487538
600 users tested
Score so far: 498.4268891693222
700 users tested
Score so far: 581.1334125793506
800 users tested
Score so far: 663.7179944721634
900 users tested
Score so far: 746.0474962705288
AUC: 0.828894041097185
\end{lstlisting}
These are all evaluation metrics. They work the same way for all 
recommendation algorithms. You just need to provide the \lstinline!getRec! function.

The second non-personalized algorithm is the random recommender.
You can use it like this:
\begin{lstlisting}[style=python]
>>> randomRec = recommender.nonpersonalized.randomRec(trainingDict, 1234567890)
>>> randomRec.getRec(0, 10)
[1548, 1068, 746, 1357, 1501, 1359, 428, 141, 233, 726]
>>> util.test.hitrate(evaluationDict, randomRec.getRec, 10)
0 users tested
Hits so far: 0.0
100 users tested
Hits so far: 2.0
200 users tested
Hits so far: 3.0
300 users tested
Hits so far: 3.0
400 users tested
Hits so far: 4.0
500 users tested
Hits so far: 4.0
600 users tested
Hits so far: 4.0
700 users tested
Hits so far: 4.0
800 users tested
Hits so far: 5.0
900 users tested
Hits so far: 5.0
Number of hits: 5.0
Number of possible hits: 943.0
Hitrate: 0.005302226935312832
\end{lstlisting}
The second argument of the constructor is the seed for the random function.

From now on, we will only show the hitrate, because the other metrics are computed
similarly. But feel free to also compute the other metrics as well.

\section{k-Nearest Neighbor}
Now we want to try the first more sophisticated algorithm, namely the item based
k-Nearest Neighbor. The k-Nearest Neighbor algorithms need the database as a matrix so we have to
first split the matrix, provided by the \lstinline!reader! object.
\begin{lstlisting}[style=python]
>>> trainingMatrix, matrixEvaluationDict = util.split.splitMatrix(r.getMatrix, 123456789)
0 Users split.
100 Users split.
200 Users split.
300 Users split.
400 Users split.
500 Users split.
600 Users split.
700 Users split.
800 Users split.
900 Users split.
\end{lstlisting}
Depending on the recommendation algorithm, we need a matrix or a dict.
So here we pass a matrix like \lstinline!r.getMatrix()! returns and get a trainingMatrix, where one
entry per user is set to zero and a dict with these missing entries.
To understand the matrix represention of the dataset refer to \ref{itembasedknn}.

Thus we initialize an itemKnn object. The initialization builds the model yet,
so it will need some time until the constructor has finished.
\begin{lstlisting}[style=python]
>>> import recommender.knn
>>> itemKnn = recommender.knn.itemKnn(trainingMatrix, 10)
0 Similarities calculated
10000 Similarities calculated
20000 Similarities calculated
30000 Similarities calculated
...
1400000 Similarities calculated
1410000 Similarities calculated
\end{lstlisting}
Please note that you pass the the right dict for evaluation as the first argument. 
In this case it is \lstinline!matrixEvaluationDict!.
The second argument is the number of neighbors to include into the neighborhood.
During the model building, phase the algorithm will compute the similarity of each
item to each other item. But because this similarity is symmetric, it is only
necessary to compute the half. The 100k movieLens dataset we use for these examples
has 1682 items, so we have to compute \begin{math} \frac{N(N-1)}{2}=\frac{1682*1681}{2} = 1413721 \end{math} 
similarities.

Now you can test this algorithm with the test metrics for example the hitrate:
\begin{lstlisting}[style=python]
>>> util.test.hitrate(matrixEvaluationDict, itemKnn.getRec, 10)
0 users tested
Hits so far: 0.0
100 users tested
Hits so far: 21.0
200 users tested
Hits so far: 54.0
300 users tested
Hits so far: 78.0
400 users tested
Hits so far: 97.0
500 users tested
Hits so far: 126.0
600 users tested
Hits so far: 152.0
700 users tested
Hits so far: 181.0
800 users tested
Hits so far: 211.0
900 users tested
Hits so far: 239.0
Number of hits: 248.0
Number of possible hits: 943.0
Hitrate: 0.26299045599151644
\end{lstlisting}
As you can see, this algorithm achieves a much better performance than the
non-personalized algorithms.

For user based k-Nearest Neighbors, the call to build the model looks like this:
\begin{lstlisting}[style=python]
>>> userKnn = recommender.knn.userKnn(trainingMatrix, 50)
0 Similarities calculated
10000 Similarities calculated
20000 Similarities calculated
30000 Similarities calculated
...
430000 Similarities calculated
440000 Similarities calculated
\end{lstlisting}
The dataset has 943 users, so this time we have to compute 
\begin{math} \frac{N(N-1)}{2}=\frac{943*942}{2} = 444153 \end{math} similarities.
To get the hitrate for this algorithm, we call the hitrate
function with the \lstinline!getRec! function just like before.
\begin{lstlisting}[style=python]
>>> util.test.hitrate(matrixEvaluationDict, userKnn.getRec, 10)
0 users tested
Hits so far: 0.0
100 users tested
Hits so far: 21.0
200 users tested
Hits so far: 52.0
300 users tested
Hits so far: 82.0
400 users tested
Hits so far: 102.0
500 users tested
Hits so far: 130.0
600 users tested
Hits so far: 153.0
700 users tested
Hits so far: 173.0
800 users tested
Hits so far: 206.0
900 users tested
Hits so far: 237.0
Number of hits: 247.0
Number of possible hits: 943.0
Hitrate: 0.26193001060445387
0.26193001060445387
\end{lstlisting}

\section{BPRMF}
For matrix factorization algorithms like BPRMF, RankMFX
and Ranking SVD, you need to specify some meta parameters.
\begin{lstlisting}[style=python]
>>> import recommender.BPRMF
>>> W, H = recommender.BPRMF.learnModel(r.getMaxUid(), r.getMaxIid(),
                        0.01, 0.01, 0.01,   # regularization parameter
                        0.1,                # learning rate
                        trainingDict,       # training dict
                        150,                # number of features
                        3,                  # number of epochs
                        r.numberOfTransactions)
Epoch: 1/3 | iteration 1000/100000 | learning rate=0.100000 | average_loss for the last 1000 iterations = 0.697530
Epoch: 1/3 | iteration 2000/100000 | learning rate=0.100000 | average_loss for the last 1000 iterations = 0.694108
Epoch: 1/3 | iteration 3000/100000 | learning rate=0.100000 | average_loss for the last 1000 iterations = 0.695815
...
Epoch: 3/3 | iteration 99000/100000 | learning rate=0.100000 | average_loss for the last 1000 iterations = 0.124050
Epoch: 3/3 | iteration 100000/100000 | learning rate=0.100000 | average_loss for the last 1000 iterations = 0.125070
\end{lstlisting}
This will output quite much, so you can observe how the loss is behaving.
With a short search, we found these values for the regularization parameters
and the learning rate to be quite good, but they are probably not the
best. But above all, the results will get better with more epochs, but then
it will, of course, take longer to compute. Thus, three is alright for a short
experiment.\\
The \lstinline!learnModel! returns a model in the form of two matrices. Two get a 
\lstinline!getRec! method, you have to instantiate a \lstinline!Mfrec! class from the \lstinline!mf! module
in \lstinline!recommender.mf!, which offers such a method.
\begin{lstlisting}[style=python]
>>> BPRMF = recommender.mf.MFrec(W, H, trainingDict)
>>> util.test.hitrate(evaluationDict, BPRMF.getRec, 10)
0 users tested
Hits so far: 0.0
100 users tested
Hits so far: 26.0
200 users tested
Hits so far: 51.0
300 users tested
Hits so far: 78.0
400 users tested
Hits so far: 95.0
500 users tested
Hits so far: 122.0
600 users tested
Hits so far: 148.0
700 users tested
Hits so far: 171.0
800 users tested
Hits so far: 203.0
900 users tested
Hits so far: 224.0
Number of hits: 235.0
Number of possible hits: 943.0
Hitrate: 0.2492046659597031
\end{lstlisting}
As noted above, the results get better with more epochs.
\section{RankMFX}
\begin{lstlisting}[style=python]
>>> import recommender.RankMFX
>>> W, H = recommender.RankMFX.learnModel(r.getMaxUid(),r.getMaxIid(),
                    0.01, 0.01, 0.01, # regularization parameter
                    0.1,              # learning rate
                    trainingDict,     # training dict
                    250,              # number of features
                    5,                # number of epochs
                    r.numberOfTransactions)
Epoch: 1/5 | iteration 1000/100000 | learning rate=0.100000 | average_loss for the last 1000 iterations = 0.984394
Epoch: 1/5 | iteration 2000/100000 | learning rate=0.100000 | average_loss for the last 1000 iterations = 0.978983
Epoch: 1/5 | iteration 3000/100000 | learning rate=0.100000 | average_loss for the last 1000 iterations = 0.966176
...
Epoch: 5/5 | iteration 99000/100000 | learning rate=0.100000 | average_loss for the last 1000 iterations = 0.141662
Epoch: 5/5 | iteration 100000/100000 | learning rate=0.100000 | average_loss for the last 1000 iterations = 0.130552
\end{lstlisting}
Again, there is much output to monitor the algorithm.
As above, we have to generate a \lstinline!getRec! method before we can evaluate the algorithm.
\begin{lstlisting}[style=python]
>>> RankMFX = recommender.mf.MFrec(W, H, trainingDict)
>>> util.test.hitrate(evaluationDict, RankMFX.getRec, 10)
0 users tested
Hits so far: 0.0
100 users tested
Hits so far: 20.0
200 users tested
Hits so far: 37.0
300 users tested
Hits so far: 54.0
400 users tested
Hits so far: 73.0
500 users tested
Hits so far: 103.0
600 users tested
Hits so far: 123.0
700 users tested
Hits so far: 136.0
800 users tested
Hits so far: 154.0
900 users tested
Hits so far: 172.0
Number of hits: 177.0
Number of possible hits: 943.0
Hitrate: 0.18769883351007424
\end{lstlisting}
What applied to BPRMF in terms of performance also applies to RankMFX:
With better tuned meta arguments and more time, the results will likely
be better.


\section{Ranking SVD (Sparse SVD)}
Next, the Ranking SVD or Sparse SVD algorithm: To build the model, we have 
a method similar to RankMFX and BPRMF, just without regularization parameters.
\begin{lstlisting}[style=python]
>>> import recommender.svd
>>> W, H = recommender.svd.learnModel(r.getMaxUid(), r.getMaxIid(),
                    0.0002,         # learning rate
                    trainingDict,   # training dict
                    770,            # number of features
                    40,             # number of epochs
                    1000)           # number of iterations
Epoch: 1/40 | iteration 100/1000 | learning rate=0.000200 | average_loss for the last 100 iterations = -3.451531 | time needed: 0.420000
Epoch: 1/40 | iteration 200/1000 | learning rate=0.000200 | average_loss for the last 100 iterations = -3.687917 | time needed: 0.400000
Epoch: 1/40 | iteration 300/1000 | learning rate=0.000200 | average_loss for the last 100 iterations = -3.433482 | time needed: 0.410000
...
Epoch: 40/40 | iteration 900/1000 | learning rate=0.000200 | average_loss for the last 100 iterations = -2.934971 | time needed: 0.410000
Epoch: 40/40 | iteration 1000/1000 | learning rate=0.000200 | average_loss for the last 100 iterations = -2.734519 | time needed: 0.400000
\end{lstlisting}
The evaluation works the same way, as for the other matrix factorization algorithms:
\begin{lstlisting}[style=python]
>>> svd = recommender.mf.MFrec(W, H, trainingDict)
>>> util.test.hitrate(evaluationDict, svd.getRec, 10)
0 users tested
Hits so far: 0.0
100 users tested
Hits so far: 7.0
200 users tested
Hits so far: 19.0
300 users tested
Hits so far: 29.0
400 users tested
Hits so far: 37.0
500 users tested
Hits so far: 54.0
600 users tested
Hits so far: 71.0
700 users tested
Hits so far: 77.0
800 users tested
Hits so far: 95.0
900 users tested
Hits so far: 108.0
Number of hits: 113.0
Number of possible hits: 943.0
Hitrate: 0.11983032873807
\end{lstlisting}

At last the slope one recommender:
\begin{lstlisting}[style=python]
>>> import recommender.slopeone
>>> slopeone = recommender.slopeone.slopeone(trainingDict)
100000 differences computed
200000 differences computed
300000 differences computed
...
19800000 differences computed
19900000 differences computed
\end{lstlisting}

And its evaluation:
\begin{lstlisting}[style=python]
>>> util.test.hitrate(evaluationDict, slopeone.getRec, 10)
0 users tested
Hits so far: 0.0
100 users tested
Hits so far: 11.0
200 users tested
Hits so far: 21.0
300 users tested
Hits so far: 28.0
400 users tested
Hits so far: 38.0
500 users tested
Hits so far: 49.0
600 users tested
Hits so far: 61.0
700 users tested
Hits so far: 70.0
800 users tested
Hits so far: 76.0
900 users tested
Hits so far: 78.0
Number of hits: 81.0
Number of possible hits: 943.0
Hitrate: 0.08589607635206786
\end{lstlisting}
