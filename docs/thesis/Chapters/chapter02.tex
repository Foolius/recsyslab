
\chapter{Background}
\label{background}
In this chapter we will provide a short overview over the techniques
used to evaluate recommender algorithms.


\section{Evaluation Methods}

To evaluate a recommender algorithm we have to split up the database
into one for training and one for evaluation. There are different
methods to split the database but in the library only one is implemented
which is the Leave-one-out protocol~\cite{leaveoneout}.
You can use the Leave-one-out protocol with many different metrics
which are also explained here.


\subsection{Leave-one-out Protocol}
\label{leaveoneout}

The Leave-one-out protocol works as follows:
\begin{enumerate}
\item Randomly choose one interaction by user. These are the hidden
interactions.
\item Put the hidden interactions into the test set.
\item Put all other interactions into the training set.
\item Let the recommender recommend N items for every user by ranking
all items, except the ones the user already interacted with,
and recommending the first N items .
\item If the hidden interaction of the user is in the recommendations
for this user, the recommender got a hit.
\item Count the hits and record the position of the interaction in the 
recommendations.
\item Use these informations to compute the various metrics.

\end{enumerate}
Because of the split we can test the recommender on every user.
This would not be possible
if we would for example split by randomly choosing 1\% of the interactions.
Then it would be likely that there are some users without a hidden item.
And without a hidden item it would be impossible to measure the quality
of the recommendations for this user.



\subsection{Evaluation metrics}
\label{evaluationmetrics}

These are a selection of different metrics to rate the recommendations.
By default the evaluations are executed with only one hidden item
but generally the metrics should also work with more than just one.

For the notation: U is the set of users, H is the set of hidden items
and \(H_u\) is the set of hidden items for user u. T is the set used for 
training. \(\text{TopN}_u\)
is the set of top N recommendations for user u so the number of items
the recommender is allowed to recommend is N. For the metrics where
the order in which the items are recommended count \(\text{TopN}_u\)
is a list sorted by score in decreasing order.
To get an implicit score of each item the recommender recommends all
items.


\subsubsection{Hitrate/Recall@N}

This metrics lets the recommender recommend N items. If the hidden
item is under the N recommended items, the recommender got a 
hit~\cite{Karypis:2001:EIT:502585.502627, Sarwar00applicationof}.
So the Recall@N is the fraction of users who get recommended a
relevant item when the recommender can recommend N items.
So the hitrate is 

\begin{equation} 
\text{Recall@N}=\frac{\sum_{u \in U} H_u \cap \text{topN}_u}{|H|}
\end{equation}


This metric is very intuitive you can for example imagine that you
show the user 10 items then Recall@10 would be the chance of showing
the user an item he will interact with. But this metric does not take
the number of recommended items into account.


\subsubsection{Precision}

The precision~\cite{Sarwar00applicationof} is 

\begin{equation} 
\text{Precision}=\frac{\sum_{u \in U} H_u \cap \text{topN}_u}{N \times |U|}
\end{equation}


As you can clearly see this metric is taken the number of recommended
items into account. Which will probably lead to worse results as the
number of recommended items increases.


\subsubsection{F1}

The F1 metric~\cite{Sarwar00applicationof} tries to balance hitrate and precision
by taking both into account.

\begin{equation}
\text{F1}=\frac{2 \times \text{Recall@N} \times \text{Precision}}{\text{Recall@N} + \text{Precision}}
\end{equation}


\subsubsection{Mean Reciprocal Hitrate}

The mean reciprocal hitrate or more general mean reciprocal
rank~\cite{DBLP:conf/icdm/NingK11} counts the hits but punishes them the more the lower they
appear in the list of recommendations. So if the hidden item appears
first in the list of recommendations the hit counts as one, but when
it is in the second position the hit already counts only as one half
and so on.

\begin{equation}
\text{MRHR}=\frac{1}{|U|} \sum_{u \in U} \frac{1}{\text{pos}(\text{topN}_{u},H_{u})}
\end{equation}

Where N is the number of items in the dataset and \(\text{pos}(\text{topN}_{u},H_{u})\)
is the position of the hidden item in the recommendation.


\subsubsection{Area under the ROC (AUC)}

AUC~\cite{Rendle:2009:BBP:1795114.1795167} counts the number of items the recommender rates
higher than the hidden item, normalize it by the number of items the
recommender can rate higher. Sum this up for every user and again
normalize by the number of users.

To get an implicit score of each item the recommender recommends all
items in a list sorted by score in decreasing order. This is in fact the same
as for the other metrics only that the recommender can recommend as
many items as possible.

\begin{equation}
\text{AUC}=\frac{1}{|U|}\sum_{u \in U} \frac{1}{|E(u)|} \sum_{(i,j) \in E(u)} \delta(x_{ui}>x_{uj})
\end{equation}

Where \(x_{ui}\) is the predicted score of the interaction between User u and item i.
\(\delta\) is defined as follows
\begin{equation}
\delta(x)=\begin{cases}1, & \text{if x is true} \\
                       0, & \text{otherwise}
\end{cases}
\end{equation}

And \(E(u)\) is 
\begin{equation}
E(u) =\{(i,j)|(u,i) \in H \land (u,j) \not\in (H \cup T)\}
\end{equation}


\section{Datasets for testing}

In the WWW there are several anonymized datasets available to try
out recommender systems and to evaluate their performance. 
Following we will introduce three of them.


\subsection{MovieLens}
\label{movielens}

MovieLens~\cite{movielensdatasets} is a database provided by GroupLens, a research
lab at the University of Minnesota. One of their research areas is
recommender systems and they built an application where users rate
movies and then get recommendations for movies the could like. The
MovieLens dataset is the ratings gathered by this application. For
this work we will interpret the rating as intensity of interaction
between users and items for example the number of times the user saw
this movie.

The dataset is available in three different sizes:
\begin{itemize}
\item 100,000 interactions
\item 1 million interactions
\item 10 million interactions
\end{itemize}
For the experiments the smallest dataset is totally sufficient, with
the larger datasets the computation time gets too long for just trying
something out.


\subsection{Million Song Dataset}
The million song dataset~\cite{Bertin-Mahieux2011} is a large database of features and media data
of a million songs. For a challenge they also provided the listening history of over 1 million
user. To present I will use a subset of this dataset to keep the computing time required
reasonable low so it is easier for others to retrace the results.


\subsection{476 Million Twitter Tweets Dataset}
The Stanford Network Analysis Project provided a twitter dataset with about 467 million tweets from 17.000 users~\cite{snap}.
Unfortunately the dataset is no more available. [further explanation or deletion]
To convert the tweets two user item interactions I will interpret the hashtags[explanation necessary?] as items.
So tweets of a user with a hashtag is a interaction between the user and the hashtag.
