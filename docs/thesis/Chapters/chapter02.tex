
\chapter{Background}
\label{background}
In this chapter, we provide a short overview of the recommender systems realm, and one particular protocol to evaluate them.


\section{Collaborative Filtering}
Broadly speaking, there are two approaches for recommender systems:
\begin{itemize}
\item Collaborative filtering
\item Content-based filtering
\end{itemize}

Collaborative filtering is, in general, the process of finding 
information or patterns in large datasets to predict user preferences. By assuming that users who 
have the same opinion on some items will also agree on others, collaborative filtering models are capable of generating predictions without using additional information like the age of 
the users~\cite{collaborativefiltering}.

Content-based filtering, on the other hand, uses information about users and items.
For example, the age of the users or the name or genre of the items~\cite{contentbasedfiltering}.

This work focuses only on collaborative filtering. So when we are 
speaking about recommender systems we mean recommender systems
using the collaborative filtering approach.


\section{Matrix Factorization}
\label{matrixfactorization}
An important class of recommender algorithms for collaborative filtering uses matrix factorization. 
Matrix factorization approximates a large and sparse matrix into a product of two low-rank matrices~\cite{matrixfactorization}.

In recommender systems, the matrix representing the dataset of user-item interactions ($M$) is factorized into
two matrices \(W\) and \(H\) such that \(\hat{M} = W\;H^\top\).
The matrices \(W\) and \(H\) are supposed to represent abstract
features of each item and user correspondingly. For recommendation, the dot product
of the feature vector of an user and an item gives a score, with which
we can sort the items and recommend the most suitable ones. 


\section{Evaluation Methods}

To evaluate a recommender algorithm we have to split up the database
into one for training and one for evaluation. There are different
methods to split the database, but in the library only one is implemented,
which is the leave-one-out protocol~\cite{leaveoneout}.
One can use the leave-one-out protocol with many different metrics
which are also explained here.


\subsection{Leave-one-out Protocol}
\label{leaveoneout}

The leave-one-out protocol works as follows:
\begin{enumerate}
\item Randomly choose one interaction for every user. These are the hidden
interactions.
\item Put the hidden interactions into the test set.
\item Put all other interactions into the training set.
\item Let the recommender recommend the first N items for every user by ranking
all items, except the ones the user already interacted with.
\item If the hidden interaction of the user is in the recommendations
for this user, the recommender gets a hit.
\item Count the hits and record the position of the interaction in the 
recommendations.
\item Use this information to compute the various metrics.

\end{enumerate}
Because of the split, we can test the recommender on every user.
This would not be possible
if we would, for example, split by randomly choosing 1\% of the interactions.
Then it would be likely that there are some users without a hidden item.
And without a hidden item it would be impossible to measure the quality
of the recommendations for this user.




\subsection{Evaluation metrics}
\label{evaluationmetrics}

These are a selection of different metrics to rate the recommendations.
By default the evaluations are executed with only one hidden item.
But generally, the metrics should also work with more than just one.

For the notation: $U$ is the set of users, $H$ is the set of hidden items
and \(H_u\) is the set of hidden items for user $u$. $T$ is the set used for 
training. \(\text{TopN}_u\)
is the set of top N recommendations for user $u$ so the number of items
the recommender is allowed to recommend is N. For the metrics, where
the order in which the items are recommended count, \(\text{TopN}_u\)
is a list sorted by score in decreasing order.
To get an implicit score of each item the recommender recommends all
items.

The recommender algorithm recommends N items. This N has to be chosen
according to the deployment scenario. For example, the number of items that can get recommended at once depends on the layout of the website.
According to a chosen N and with the metrics of recsyslab, the right
algorithm can be found. 

Except for AUC and MRHR, the metrics, defined later in this section, do not take the position
of the items in the recommendation list into account.

Whether the position is important or not, depends on the use case.
When all recommended items are presented equally well to the
user, the position of the items in the recommendation
is not as important as whether or not the right items are
recommended. But in contrast to this, when the user is shown 
a long list of items and it is likely that the user will not
scroll down until the end of the list, one would rather care about
the position of the items in the list of recommendations.

\subsubsection{Hitrate/Recall@N}

This metric lets the recommender recommend N items. If the hidden
item is under the N recommended items, the recommender gets a 
hit~\cite{Karypis:2001:EIT:502585.502627, Sarwar00applicationof}.
So the Recall@N is the fraction of users who get recommended a
relevant item when the recommender can recommend N items.
So the hitrate is 

\begin{equation} 
\text{Recall@N}=\frac{\sum_{u \in U} H_u \cap \text{topN}_u}{|H|}.
\end{equation}
This metric is very intuitive. For example, imagine that you
show the user 10 items, then Recall@10 would be the chance of showing
the user an item he will interact with. But this metric does not take
the number of recommended items into account.


\subsubsection{Precision}

The precision~\cite{Sarwar00applicationof} is defined as

\begin{equation} 
\text{Precision}=\frac{\sum_{u \in U} H_u \cap \text{topN}_u}{N \times |U|}.
\end{equation}
As one can clearly see, this metric takes the number of recommended
items into account. This will probably lead to worse results as the
number of recommended items increases.


\subsubsection{F1}

The F1 metric~\cite{Sarwar00applicationof} tries to balance hitrate and precision
by taking both into account. It is defined as

\begin{equation}
\text{F1}=\frac{2 \times \text{Recall@N} \times \text{Precision}}{\text{Recall@N} + \text{Precision}}.
\end{equation}


\subsubsection{Mean Reciprocal Hitrate}

The mean reciprocal hitrate (MRHR), or the more general mean reciprocal
rank~\cite{DBLP:conf/icdm/NingK11}, counts the hits but punishes them more, the lower they
appear in the list of recommendations. So if the hidden item appears
first in the list of recommendations, the hit counts as one. But when
it is in the second, position the hit counts only as one half
and so on. It is defined by

\begin{equation}
\text{MRHR}=\frac{1}{|U|} \sum_{u \in U} \frac{1}{\text{pos}(\text{topN}_{u},H_{u})},
\end{equation}
where N is the number of items in the dataset and \(\text{pos}(\text{topN}_{u},H_{u})\)
is the position of the hidden item in the recommendation.


\subsubsection{Area under the ROC (AUC)}

AUC~\cite{Rendle:2009:BBP:1795114.1795167} counts the number of items that the recommender rates
higher than the hidden item, normalized by the number of items the
recommender can rate higher. This is in turn summed up for every user and 
normalized by the number of users.

To get an implicit score of each item, the recommender recommends all
items in a list, sorted by score in a decreasing order. This is in fact the same
as for the other metrics, except that the recommender can recommend as
many items as possible. It is defined as

\begin{equation}
\text{AUC}=\frac{1}{|U|}\sum_{u \in U} \frac{1}{|E(u)|} \sum_{(i,j) \in E(u)} \delta(x_{ui}>x_{uj}),
\end{equation}
where \(x_{ui}\) is the predicted score of the interaction between User u and item i.
\(\delta\) is defined as follows
\begin{equation}
\delta(x)=\begin{cases}1, & \text{if x is true}, \\
                       0, & \text{otherwise.}
\end{cases}
\end{equation}
And \(E(u)\) is 
\begin{equation}
E(u) =\{(i,j)|(u,i) \in H \land (u,j) \not\in (H \cup T)\}.
\end{equation}


%\section{Datasets for testing}
%
%In the WWW there are several anonymized datasets available to try
%out recommender systems and to evaluate their performance. 
%Following we will introduce three of them.


%\section{Datasets for testing}
%\subsection{MovieLens}
%\label{movielens}

%MovieLens~\cite{movielensdatasets} is a database provided by GroupLens, a research
%lab at the University of Minnesota. One of their research areas is
%recommender systems and they built an application where users rate
%movies and then get recommendations for movies the could like. The
%MovieLens dataset is the ratings gathered by this application. For
%this work we will interpret the rating as intensity of interaction
%between users and items for example the number of times the user saw
%this movie.

%The dataset is available in three different sizes:
%\begin{itemize}
%\item 100,000 interactions
%\item 1 million interactions
%\item 10 million interactions
%\end{itemize}
%For the experiments the smallest dataset is totally sufficient, with
%the larger datasets the computation time gets too long for just trying
%something out.


%\subsection{Million Song Dataset}
%The million song dataset~\cite{Bertin-Mahieux2011} is a large database of features and media data
%of a million songs. For a challenge they also provided the listening history of over 1 million
%user. To present I will use a subset of this dataset to keep the computing time required
%reasonable low so it is easier for others to retrace the results.


%\subsection{476 Million Twitter Tweets Dataset}
%The Stanford Network Analysis Project provided a twitter dataset with about 467 million tweets from 17.000 users~\cite{snap}.
%Unfortunately the dataset is no more available. [further explanation or deletion]
%To convert the tweets two user item interactions I will interpret the hashtags[explanation necessary?] as items.
%So tweets of a user with a hashtag is a interaction between the user and the hashtag.
