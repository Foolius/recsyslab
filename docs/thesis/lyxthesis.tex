%% LyX 2.0.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{scrreprt}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\usepackage{amsmath}
\usepackage{babel}
\begin{document}

\title{Multi-purpose Library of Recommender System Algorithms for the Item
Prediction Task}


\author{Julius Kolbe}


\date{11.6.2013}

\maketitle
\tableofcontents{}


\chapter{Abstract}

In this thesis I will give an introduction to recommender systems,
provide an overview over other recommender system libraries and datasets
available to try out the algorithms. After that I will describe different
recommender algorithms and evaluation metrics I implemented in my
work followed by an explanation on how to use them. Additionally I
will provide the result of the tests.


\chapter{Introduction}


\section{Motivation}

The library together with this document shall provide a ``cookbook''
for recommender systems. With the simple syntax and the interactivity
of Python it is aimed at beginners to simply experiment with different
algorithms. Especially the interactivity is missing in the already
existing libraries because none of them is written in Python.


\section{Task (what a Recommender System does)}

A Recommender System works in a scenario with users, items and interactions
users can have with items. Such a scenario could be an online shop,
where the interactions are purchases of items by users or a video
platform, where the users interact with items (videos) by watching
them. Based on the past interactions of the users a Recommender System
searches for items a user haven't interacted with yet but the probability
that he will interact is maximized.

The interactions can be implicit like purchases or clicks, then the
scenario is also called item prediction. When the feedback is provided
explicit like ratings the scenario is called rating prediction. In
this work the focus lies on implicit feedback or item prediction.
However ratings can be interpreted as the strength of implicit feedback.
For example how often a user purchased an item. Some algorithms implemented
in this library can use this information but none will explicitly
predict ratings like it's usual in rating prediction scenarios.


\section{Contributions}

The main contribution of my work is the interactive library I wrote
\cite{key-3}. Also in this document I provide explanations about
the algorithms implemented in the library and an extensive user manual
of the library.


\chapter{Background}


\section{Evaluation Methods}

To evaluate a recommender algorithm we have to split up the database
into one for training and one for evaluation. There are different
methods to split the database but in the library only one is implemented.


\subsection{Leave-one-out Protocol}

The Leave-one-out Protocol means, that we take one interaction of
each user out of the database for training and use it for validation.
The item the recommender has to predict is also called hidden item. 

Now we can test for each user if the algorithm is capable to predict
this missing interaction.


\subsection{Evaluation metrics}

These are a selection of different metrics to rate the recommendations.
By default the evaluations are executed with only one hidden item
but generally the metrics should also work with more than just one.


\subsubsection{Hitrate/Recall@N}

This metrics lets the recommender recommend N items. If the hidden
item is under the N recommended items, the recommender got a hit \cite{key-4,key-5}.
So the hitrate is 

\[
\mathrm{\textrm{Hitrate}}=\frac{\textrm{\ensuremath{\mathrm{Number}}ofhits}}{\textrm{Numberofhiddenitems}}
\]


This metric is very intuitive you can for example imagine that you
show the user 10 items then Recall@10 would be the chance of showing
the user an item he will interact with. But this metric doesn't take
the number of recommended items into account.


\subsubsection{Precision}

The precision\cite{key-5} is 

\[
\textrm{Precision}=\frac{\textrm{numberofhits}}{\textrm{numberofrecommendeditems}}
\]


As you can clearly see this metric is taken the number of recommended
items into account. Which will probably lead to worse results as the
number of recommended items increases.


\subsubsection{F1}

The F1 metric\cite{key-5} tries to balance hitrate and precision
by taking both into account.

\[
\textrm{F1}=\frac{2*\textrm{Hitrate}*\textrm{Precision}}{(\textrm{Recall}+\textrm{Precision})}
\]
 


\subsubsection{Mean Reciprocal Hitrate}

This metric counts the hits but punishes them the more the lower they
appear in the list of recommendations. So if the hidden item appears
first in the list of recommendations the hit counts as one, but when
it is in the second position the hit already counts only as one half
and so on. 


\subsubsection{Area under the ROC (AUC)}

AUC\cite{key-10} counts the number of items the recommender rates
higher than the hidden item, normalize it by the number of items the
recommender can rate higher. Sum this up for every user and again
normalize by the number of users.

To get an implicit score of each item the recommender recommends all
items in a list sorted by decreasing score. This is in fact the same
as for the other metrics only that the recommender can recommend as
many items as possible.


\section{Datasets for testing}

In the WWW there are several anonymized datasets available to try
out recommender systems. Following I will introduce three of them.


\subsection{MovieLens}

MovieLens\cite{key-12} is a database provided by GroupLens, a research
lab at the University of Minnesota. One of their research areas is
recommender systems and they built an application where users rate
movies and then get recommendations for movies the could like. The
MovieLens dataset is the ratings gathered by this application. For
this work I will interpret the rating as intensity of interaction
between users and items for example the number of times the user saw
this movie.

The dataset is available in three different sizes:
\begin{itemize}
\item 100,000 ratings
\item 1 million ratings
\item 10 million ratings
\end{itemize}
For the experiments the smallest dataset is totally sufficient, with
the larger datasets the computation time gets too long for just trying
something out.


\subsection{Million Song Dataset}


\subsection{SNAP}


\chapter{Related Work}

There is a wide range of projects providing implemantions for recommender
system. Some of them are described in this chapter to give a quick
overview and comparison.


\section{MyMediaLite}

MyMediaLite\cite{key-1} is an open source project developed at the
University of Hildesheim and provides several algorithm for rating
prediction and item prediction. It is written in C\# and is used with
a command line interface. It also provides a graphical interface to
demonstrate recommender algorithms


\section{PREA (Personalized Recommendation Algorithms Toolkit)}

PREA\cite{key-2} is an open source project written in Java. It provides
a wide range of recommender algorithms and evaluation metrics to test
them. It is maintained by the Georgia Institute of Technology.


\section{Apache Mahout}

Mahout\cite{key-6} is an open source library in java. It is implemented
on top of Apache Hadoop, so it uses the map/reduce paradigm. This
means it can run on different independent computers.


\section{Duine Framework}

The Duine Framework \cite{key-7} is an open source project written
in java by the Telematica Instituut/Novay. The recommender of the
Duine Framework combines multiple prediction techniques to exploit
the strengths of the different techniques and to avoid their weaknesses.


\section{Cofi}

Cofi \cite{key-8}provides an algorithm for the rating prediction
task called Maximum Margin Matrix Factorization. It is open source
and written in C++.


\section{LensKit}

Lenskit \cite{key-9} is a toolkit which provides several recommender
algorithms and an infrastructure to evaluate them. It is an open source
project by the University of Minnesota


\chapter{Recommendation Algorithms}


\section{Non-Personalized Algorithms}

In this chapter I will describe two very simple and basic recommendation
algorithms I implemented for comparison with the more sophisticated
algorithms.


\subsection{Constant}

The constant recommender algorithm counts the number of interactions
for each item and sorts this in decreasing order of interactions.
Then it recommends the top items of this list. So it recommends the
items which are the most popular over all users and doesn't do any
personalizations.


\subsection{Random}

The random recommender just picks items randomly.


\section{k-Nearest-Neighbor}

This class of recommendation algorithms works by searching neighbors
of either items or users based on a similarity function which is the
cosine in this library.


\subsection{Item Based}

For this algorithm the database has to be represented as a matrix
where the rows correspond to the users and the columns to the items.
Then the entry (i,j) represents the number of transactions which happened
between the ith user and the jth item. 

The algorithm interprets the columns of the matrix i.e. the items
as vectors and computes there similarities by computing their cosine.
To build the model the algorithm computes the n most similar items
of each item. 

To compute recommendations for user U the algorithm then computes
the union of the n most similar items of each item U interacted with.
From this set the items U already interacted with are removed. For
each item remaining in this set we compute the sum of its similarities
to the items U interacted with. Finally these items are sorted in
decreasing order of this sum of similarities and the first n items
will be recommended\cite{key-4}.


\subsection{User Based}

The user based k-Nearest-Neighbor is very similar to the item based.
But instead of interpreting the columns as vectors we interpret the
lines or users of the matrix as vectors and compute their similarities
to other users.

Then for each item i we sum up the similarities between U and the
users who interacted with i. Again we remove all items U already interacted
with, sort in decreasing order fo the sum and recommend the first
n items.


\section{Matrix Factorization}

All matrix factorization techniques build two matrices in the model
building phase. These matrices are supposed to represent abstract
features of each item and user. For recommendation the dot product
of the feature vector of an user and an item gives a score whith which
we can sort the items and recommend the best suitable ones. The process
of presenting a large matrix M as two smaller matrices W and H so
that M = W{*}H is also called singular value decomposition.

Each of the implemented algorithms train the model with stochastic
gradient descent. In each iteration the model is trained with a randomly
chosen user, a randomly chosen item the user interacted with, called
the positive item and a randomly chosen item the user didn't interacted
with yet, called the negative item. The features of the user and the
negative and the positive item are then trained according to the derivative
of a loss function.


\subsection{BPRMF}

BPMRF uses the logloss to train the model. The logloss is defined
as
\[
\textrm{logLoss}(a,y)=\log(1+\exp(-ay))
\]


And the derivative of the log loss is
\[
\frac{\partial}{\partial y}(\log(1+\exp(-ay)))=-\frac{a}{\exp(ay)+1}
\]


For further informations please refer to \cite{key-10}


\subsection{RankMFX}

RankMFX uses the hingeLoss. It is defined as

\[
\mathrm{\textrm{hingeLoss}(a,y)=\max(0,1-ay)}
\]


And its derivative

\[
\frac{\partial}{\partial y}(\max(0,1-ay))=\begin{cases}
-y & ay<1\\
0 & \textrm{otherwise}
\end{cases}
\]


See also {[}Paper for citation?{]}


\subsection{Ranking SVD (Sparse SVD)}

Ranking SVD uses the quadratic loss and the difference between the
predicted score of the positive item and the negative minus the actual
score of the positive item.\cite{key-11}


\chapter{Experiments}


\section{Execution}


\section{Results}


\section{Comparison}


\chapter{Design and Implementation}


\section{General structure}


\section{Interfaces}


\chapter{User Manual}


\section{Primitive Algorithms}


\section{k-Nearest Neighbor}


\section{BPRMF}


\section{RankMFX}


\section{Ranking SVD (Sparse SVD)}


\chapter{Conclusions}


\section{Future work}


\section{Outlook}


\chapter{References}
\begin{thebibliography}{10}
\bibitem{key-1}Zeno Gantner, Steffen Rendle, Christoph Freudenthaler,
Lars Schmidt-Thieme: MyMediaLite: A Free Recommender System Library.
RecSys 2011

\bibitem{key-2}http://prea.gatech.edu/

\bibitem{key-6}http://mahout.apache.org/

\bibitem{key-7}http://www.duineframework.org/

\bibitem{key-8}https://sites.google.com/a/cofirank.org/index/

\bibitem{key-9}http://lenskit.grouplens.org/

\bibitem{key-3}https://github.com/Foolius/recsyslab

\bibitem{key-4}Evaluation of Item-Based Top-N Recommendation Algorithms
by George Karypis

\bibitem{key-5}Application of Dimensionality Reduction in Recommender
System -- A Case Study by Badrul M. Sarwar et al

\bibitem{key-10}BPR: Bayesian Personalized Ranking from Implicit
Feedback from Steffen Rendle et al

\bibitem{key-11}Collaborative Filtering Ensemble for Ranking by Michael
Jahrer, Andreas Töscher

\bibitem{key-12}http://grouplens.org/node/73\end{thebibliography}

\end{document}
